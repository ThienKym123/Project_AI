# -*- coding: utf-8 -*-
"""Text_Classification_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1la7EN1QNR5apqyOELsoT3QvT63kFb46r

# 1. Khai báo các thư viện cần sử dụng
"""

import matplotlib.pyplot as plt
import os
import re
import shutil
import string
import matplotlib.pyplot as plt
import tensorflow as tf

from wordcloud import WordCloud
from tensorflow.keras import layers
from tensorflow.keras import losses
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

"""# 2. Thu thập dữ liệu

Tải tập dữ liệu ý kiến của người xem về phim trên IMDb
"""

url = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"

dataset = tf.keras.utils.get_file("aclImdb_v1", url,
                                    untar=True, cache_dir='.',
                                    cache_subdir='')

dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')

"""Xem các tệp và thư mục trong tập dữ liệu

# 3. Khám phá dữ liệu
"""

os.listdir(dataset_dir)

"""Tạo thư mục chứa tập dùng để Huấn luyện và Thử nghiệm dữ liệu"""

train_dir = os.path.join(dataset_dir, 'train')

test_dir = os.path.join(dataset_dir, 'test')

"""Đọc 1 ý kiến tích cực"""

sample_file = os.path.join(train_dir, 'pos/10000_8.txt')
with open(sample_file) as f:
  print(f.read())

"""# 4. Chuẩn bị dữ liệu

Xóa bỏ thư mục không cần thiết
"""

remove_dir = os.path.join(train_dir, 'unsup')

shutil.rmtree(remove_dir)

"""Tạo tập dữ liệu dùng để huấn luyện từ thư mục Train"""

batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='training',
    seed=seed)

"""Khám phá dữ liệu ý kiến và kí hiệu nhãn dán"""

for text_batch, label_batch in raw_train_ds.take(1):
  for i in range(3):
    print("Review", text_batch.numpy()[i])
    print("Label", label_batch.numpy()[i])

"""Làm rõ tên nhãn dán"""

print("Label 0 corresponds to", raw_train_ds.class_names[0])
print("Label 1 corresponds to", raw_train_ds.class_names[1])

"""Vẽ WordCloud tổng hợp từ vựng từ các ý kiến trong 2 thư mục Neg và Pos"""

def plot_word_cloud(texts, labels):
    # Tạo WordCloud cho văn bản có nhãn 0 (tích cực)
    combined_text = ' '.join([text.numpy().decode('utf-8') for text, label in zip(texts, labels) if label == 0])
    wordcloud_neg = WordCloud(width=800, height=400, background_color='white').generate(combined_text)

    # Tạo WordCloud cho văn bản có nhãn 1 (tiêu cực)
    combined_text = ' '.join([text.numpy().decode('utf-8') for text, label in zip(texts, labels) if label == 1])
    wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(combined_text)

    # Vẽ hai WordClouds trên cùng một hình
    fig, axs = plt.subplots(1, 2, figsize=(15, 7))

    # WordCloud cho nhãn 0
    axs[0].imshow(wordcloud_neg, interpolation='bilinear')
    axs[0].set_title('WordCloud for Negative Sentiment', fontsize=15)
    axs[0].axis('off')

    # WordCloud cho nhãn 1
    axs[1].imshow(wordcloud_pos, interpolation='bilinear')
    axs[1].set_title('WordCloud for Positive Sentiment', fontsize=15)
    axs[1].axis('off')

    # Hiển thị biểu đồ
    plt.show()

# Hàm này tạo WordCloud cho tất cả các văn bản trong tập huấn luyện và vẽ chúng
train_texts = []
train_labels = []
for text_batch, label_batch in raw_train_ds:
    train_texts.extend(text_batch)
    train_labels.extend(label_batch)

plot_word_cloud(train_texts, train_labels)

"""Tạo tập dữ liệu kiểm định từ thư mục Train"""

raw_val_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train',
    batch_size=batch_size,
    validation_split=0.2,
    subset='validation',
    seed=seed)

"""Tạo tập dữ liệu Kiểm thử từ thư mục Test"""

raw_test_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/test',
    batch_size=batch_size)

"""Tạo hàm chuẩn hóa dữ liệu văn bản, loại bỏ các kí tự, thẻ HTML, dấu câu"""

def custom_standardization(input_data):
    # Chuyển đổi văn bản thành chữ thường
    lowercase = tf.strings.lower(input_data)
    # Loại bỏ các thẻ HTML
    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
    # Loại bỏ dấu câu
    return tf.strings.regex_replace(stripped_html,
                                     '[%s]' % re.escape(string.punctuation),
                                     '')

"""Tạo lớp chuyển đổi Văn bản thành Vector Embeddings."""

max_features = 10000
sequence_length = 269

vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length)

"""Ánh xạ chỉ lấy dữ liệu ý kiến để thực hiện chuyển thành Vector Embeddings"""

train_text = raw_train_ds.map(lambda x, y: x)
vectorize_layer.adapt(train_text)

"""Tạo hàm trả về bộ dữ liệu gồm Vector Embedding đại diện cho từ, và nhãn dán thể hiện ý kiến tích cực, tiêu cực tương ứng"""

def vectorize_text(text, label):
  text = tf.expand_dims(text, -1)
  return vectorize_layer(text), label

"""Khám phá dữ liệu sau khi thực hiện nhúng"""

text_batch, label_batch = next(iter(raw_train_ds))
first_review, first_label = text_batch[31], label_batch[31]
print("Review", first_review)
print("Label", raw_train_ds.class_names[first_label])
print("Vectorized review", vectorize_text(first_review, first_label))

"""Khám phá các từ ngữ mà Vector Embedding tương ứng đại diện"""

print("591 ---> ",vectorize_layer.get_vocabulary()[591])
print(" 25 ---> ",vectorize_layer.get_vocabulary()[25])
print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))

"""Thực hiện nhúng các bộ dữ liệu trong các tập dữ liệu"""

train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)
test_ds = raw_test_ds.map(vectorize_text)

"""Tối ưu quá trình tải dữ liệu bằng phương pháp tải dữ liệu song song, tối ưu hóa hiệu suất."""

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""# 5. Xây dựng, đào tạo và đánh giá mô hình

Xây dựng mô hình phân loại 2 lớp nhị phân
"""

embedding_dim = 32
dense_units = 64

model = tf.keras.Sequential([
    layers.Embedding(max_features, embedding_dim),
    layers.Dropout(0.5),
    layers.GlobalAveragePooling1D(),
    layers.Dropout(0.5),
    layers.Dense(dense_units, activation='relu'),  # Sử dụng hàm kích hoạt ReLU trong lớp Dense
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

model.summary()

"""Thiết lập quy trình huấn luyện cho mô hình"""

model.compile(loss=losses.BinaryCrossentropy(),
              optimizer='adam',
              metrics=[tf.metrics.BinaryAccuracy(threshold=0.5)])

"""Tối ưu quá trình huấn luyện"""

# EarlyStopping callback
es = tf.keras.callbacks.EarlyStopping(
    patience=3,
    monitor='val_binary_accuracy',
    restore_best_weights=True
)

# ReduceLROnPlateau callback
lr = tf.keras.callbacks.ReduceLROnPlateau(
    patience=3,
    monitor='val_loss',
    factor=0.5,
    verbose=0
)

"""Tiến hành đào tạo mô hình"""

epochs = 30
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs,
    callbacks=[lr, es]
)

"""Đánh giá mô hình dựa trên tập Kiểm thử"""

loss, accuracy = model.evaluate(test_ds)

print("Loss: ", loss)
print("Accuracy: ", accuracy)

"""Lưu lại các chỉ số huấn luyện mô hình"""

history_dict = history.history
history_dict.keys()

"""Trực quan hóa biểu đồ loss của quá trình huấn luyện và validation"""

acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""Trực quan hóa biểu đồ độ chính xác của quá trình huấn luyện và validation"""

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

plt.show()

"""# Triển khai mô hình

Tạo một mô hình sử dụng lớp Vector hóa và mô hình phân loại nhị phân trước đó và lớp kích hoạt Sigmoid để đảm bảo đầu ra trong khoảng từ 0 đến 1
"""

export_model = tf.keras.Sequential([
  vectorize_layer,
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False), optimizer="adam", metrics=['accuracy']
)

# Đánh giá mô hình trên dữ liệu thử nghiệm ban đầu chưa được xử lý và in ra độ chính xác

loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)

"""Thử nghiệm với những dữ liệu mới được truyền vào"""

# Function to predict sentiment based on the trained model
def predict_sentiment_with_score(text, model):
    # Normalize the input text
    normalized_text = vectorize_layer([text])
    # Predict
    prediction = model.predict(normalized_text)[0][0]
    return prediction

# Examples to test the model
more_example_texts = [
    "The plot of the movie was intriguing and kept me engaged throughout.",
    "I was disappointed by the ending of the movie, it felt rushed and unsatisfying.",
    "The service at the hotel was terrible, the staff were rude and unhelpful.",
    "The soundtrack of the game was fantastic, it added so much to the overall experience.",
    "The user interface of the app was intuitive and easy to navigate.",
    "The quality of the product was very poor, it broke after just a few uses.",
    "The atmosphere in the restaurant was cozy and welcoming, I enjoyed my dining experience.",
]

# Testing the model with more examples
for text in more_example_texts:
    prediction_score = predict_sentiment_with_score(text, model)
    sentiment = "Positive" if prediction_score > 0.5 else "Negative"
    print(f"Text: '{text}'")
    print(f"Predicted sentiment: {sentiment}")
    print(f"Prediction score: {prediction_score}")
    print()